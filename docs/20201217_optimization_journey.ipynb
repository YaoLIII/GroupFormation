{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Journey\n",
    "Claus Brenner, 17.12.2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a model - first, in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares estimation in 1D\n",
    "Say we are given these points $b_1, b_2, b_3, b_4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# Uncomment the following two lines to get large plots.\n",
    "#plt.rcParams['figure.figsize'] = (11, 6)\n",
    "#plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Very few selected points.\n",
    "b_1d = np.array([3, 4, 5, 7])\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\");\n",
    "plt.xlim(0,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and search for the \"least squares solution\", i.e., the $x$ that has the smallest least squares distance from all $b_i$.:\n",
    "\n",
    "$$x^*=\\arg\\min_{x\\in\\mathbb{R}} \\sum_i (x-b_i)^2$$\n",
    "\n",
    "That means, we have to vary $x\\in\\mathbb{R}$ until the sum becomes minimal.\n",
    "\n",
    "The reason why we would want to do this may be e.g. that we want to find a maximum likelihood estimation of $x$, given linear measurements corrupted by Gaussian measurement errors.\n",
    "\n",
    "So we sum up a set of quadratic functions $(x-b_i)^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals_1d = np.linspace(0, 10, num=1000)\n",
    "for b in b_1d: plt.plot(x_vals_1d, (x_vals_1d-b)**2)\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.ylim(-.5,20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe I could trick somebody to think the sum must have a complex shape, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_vals_1d, ((x_vals_1d-b_1d[:,np.newaxis])**2).min(axis=0), \"black\")\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.ylim(-.5,20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but no, the sum is of course a *quadratic function*. It can't be otherwise, because adding up quadratic terms can't result in, say, cubic terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in b_1d: plt.plot(x_vals_1d, (x_vals_1d-b)**2)\n",
    "plt.plot(x_vals_1d, ((x_vals_1d-b_1d[:,np.newaxis])**2).sum(axis=0), \"black\")\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.ylim(-.5,20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sum is a quadratic function (with positive $x^2$ coefficients), it is of course also a **convex** function. Since the *domain* of $f(x)$ is *convex* (the full $\\mathbb{R}$), and the *function* $f(x)$ itself is *convex*, the minimum can be found efficiently, for example, by \"walking downhill\".\n",
    "\n",
    "In general, what we have here is a case of **convex optimization**, see e.g. Boyd and Vandenberghe (bibliography at end).\n",
    "\n",
    "In this case, it is even easier, and we need not iteratively \"walk downhill\". The quadratic function has a global minimum where the gradient is zero. In 1D, the gradient is the derivative w.r.t. $x$, so $$f'(x)=\\frac{d}{dx}\\left(\\sum_i (x-b_i)^2\\right) = \\sum_i 2(x-b_i).$$ Then, setting $\\sum_i (x-b_i) = 0$ yields $mx - \\sum_i b_i = 0$, so the result is the mean:\n",
    "\n",
    "$$x^* = \\frac{1}{m}\\sum_i b_i.$$\n",
    "\n",
    "In our example, the mean is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(b_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in b_1d: plt.plot(x_vals_1d, (x_vals_1d-b)**2)\n",
    "plt.plot(x_vals_1d, ((x_vals_1d-b_1d[:,np.newaxis])**2).sum(axis=0), \"black\")\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.axvline(x=np.mean(b_1d), c=\"k\", ls=\"--\")\n",
    "plt.ylim(-.5,20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is indicated by the dotted line. All this says is that the least squares estimate of $\\mu$ is the mean, and the mean of this four numbers is this certain value.\n",
    "\n",
    "However, if the task is to estimate an unknown value from measurements $b_i$, then it could be that the unknown value is $4$, and the measurement process added some noise, which has lead to the measurements $3$, $4$ and $5$, whereas the value $7$ is actually an outlier.\n",
    "\n",
    "\n",
    "### Checking with random samples and outliers.\n",
    "Let's say our measurement is normal distributed with $\\mu=4$ and $\\sigma=0.1$.\n",
    "(Note the points are all on a line. Some jitter in y is added for illustration only.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "n_1d_meas = 30\n",
    "b_1d_meas = rng.normal(4, .1, n_1d_meas)\n",
    "\n",
    "def report_mean(inliers, outliers):\n",
    "    both = np.concatenate((inliers, outliers))\n",
    "    plt.plot(inliers, rng.uniform(-.2,.2,len(inliers)), \"b.\")\n",
    "    plt.plot(outliers, rng.uniform(-.2,.2,len(outliers)), \"r.\")\n",
    "    plt.axvline(x=np.mean(both), c=\"silver\", ls=\"--\")\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlim(-2,10)\n",
    "    print(\"inliers %d outliers %d mean %.3f\" % (len(inliers), len(outliers), np.mean(both)))\n",
    "\n",
    "report_mean(b_1d_meas, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, our mean works nicely as an estimator for $\\mu$. Now if we have \"outliers\", which are uniformly distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1d_outl = 25\n",
    "b_1d_outl = rng.uniform(-2,10,n_1d_outl)\n",
    "report_mean(b_1d_meas, b_1d_outl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our estimator may be influenced by the outliers. In this case, the influence is not very large, since \"data\" and \"outliers\" both are centered at the same $\\mu$.\n",
    "\n",
    "A more realistic case could assume that all outliers are on one side. E.g., in Laserscanning, the measured ranges may be mostly too short, but (almost) never too long. So now, we assume the outliers are only to the right, uniformly distributed. In this \"asymmetric\" case we get a skewed overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1d_outl_a = np.abs(b_1d_outl-4)+4\n",
    "report_mean(b_1d_meas, b_1d_outl_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least absolute deviations: the $l_1$ norm\n",
    "In least squares, outliers have a large influence on the result, because the value added to the sum increases quadratically.\n",
    "\n",
    "On can play with the function which \"weights\" how small and large deviations influence the estimation result. One can generalize\n",
    "\n",
    "$$x^* = \\arg\\min_x \\sum_i (x-b_i)^2$$\n",
    "to\n",
    "$$x^* = \\arg\\min_x \\sum_i \\rho(x-b_i)$$\n",
    "\n",
    "where $\\rho(r)=r^2$ for the least squares case (define $r$ to be the residual, $r_i=(x-b_i)$). $\\rho$ is called a **loss function** and these estimators are called **M-estimators**, see [@wikipedia](https://en.wikipedia.org/wiki/M-estimator).\n",
    "\n",
    "For example, in order to make the influence of large values smaller, and small values larger, one can use the function $\\rho(r)=|r|$, leading to:\n",
    "\n",
    "$$x^* = \\arg\\min_x \\sum_i |x-b_i|,$$\n",
    "or in general,\n",
    "$$x^* = \\arg\\min_x ||Ax-b||_1.$$\n",
    "\n",
    "For our initial 1D example, we now get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in b_1d: plt.plot(x_vals_1d, np.abs(x_vals_1d-b))\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.ylim(-.5,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can again guess the shape of the resulting sum. Will it be convex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in b_1d: plt.plot(x_vals_1d, np.abs(x_vals_1d-b))\n",
    "plt.plot(x_vals_1d, (np.abs(x_vals_1d-b_1d[:,np.newaxis])).sum(axis=0), \"black\")\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.ylim(-.5,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It *is* convex. We could have known that beforehand, because $\\rho(x)=|x|$ is convex, and *any nonnegative weighted sum of convex functions is convex* (see Boyd & Vandenberghe, sect. 3.2.1). So there is only one global minimum.\n",
    "\n",
    "However, we see that there is a plateau between $b_1$ and $b_2$. So while there is a global minimum value, its location $x^*$ is not unique.\n",
    "\n",
    "How can we find the minimum? We can walk downhill. Say, we start at 0, so we are in \"the negative section\" of all four curves, and the slope of the black curve must be $-4$. So walking downhill means, walking to the right. If we walk to the right, the next change will occur at $b_0$, where the slope of the corresponding $|x-b_0|$ curve changes from $-1$ to $+1$, so the slope of the black curve changes from $-4$ to $-2$. We continue to the right, and at $b_1$ the slope changes from $-2$ to $0$. We are at the global minimum.\n",
    "\n",
    "If we think about it, we do not have to do all of this walking. For any given point $x$, all $b_i$ to the left of it contribute $+1$ to the slope, and all $b_i$ to the right of it contribute $-1$. So we have zero if the number of $b_i$ to the left and $b_i$ to the right are equal. Therefore, the least absolute deviations estimator is the median:\n",
    "\n",
    "$$x^* = \\mbox{median}_i(d).$$\n",
    "\n",
    "If $m$, the number of observations $b_i$, is odd, the median will be in the central $b_{(m-1)/2}$, whereas if it is equal it is not uniquely defined, but usually, the mean between $b_{(m-2)/2}$ and $b_{(m-2)/2+1}$ is returned. For example, in numpy, we get the following (dotted black line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in b_1d: plt.plot(x_vals_1d, np.abs(x_vals_1d-b))\n",
    "plt.plot(x_vals_1d, (np.abs(x_vals_1d-b_1d[:,np.newaxis])).sum(axis=0), \"black\")\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.axvline(x=np.median(b_1d), c=\"k\", ls=\"--\")\n",
    "plt.ylim(-.5,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with our 1D outlier example.\n",
    "First, without outliers, the median works great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_median(inliers, outliers):\n",
    "    both = np.concatenate((inliers, outliers))\n",
    "    plt.plot(inliers, rng.uniform(-.2,.2,len(inliers)), \"b.\")\n",
    "    plt.plot(outliers, rng.uniform(-.2,.2,len(outliers)), \"r.\")\n",
    "    plt.axvline(x=np.mean(both), c=\"silver\", ls=\"--\", label=\"mean\")\n",
    "    plt.axvline(x=np.median(both), c=\"dimgray\", ls=\"--\", label=\"median\")\n",
    "    plt.legend()\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlim(-2,10)\n",
    "    print(\"inliers %d outliers %d mean %.3f median %.3f\" %\\\n",
    "          (len(inliers), len(outliers), np.mean(both), np.median(both)))\n",
    "report_median(b_1d_meas, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our \"symmetric outliers\", we usually get a better result (dark gray line) than with the mean (light gray line). (But with the small number of samples used here, it depends a bit on the actual samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_median(b_1d_meas, b_1d_outl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"asymmetric outlier\" case, the median is much better, although it starts to walk away from the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_median(b_1d_meas, b_1d_outl_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we have even more outliers, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1d_more_outl = 2*n_1d_meas\n",
    "b_1d_more_outl_a = rng.uniform(4, 10, n_1d_more_outl)\n",
    "report_median(b_1d_meas, b_1d_more_outl_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, for each two more outliers to the right, the median will \"walk one $b_i$ to the right\", no matter how far away the added outlier to the right actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Consensus: maximizing inliers, or minimizing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen that the $l_1$ norm is more robust than the $l_2$ norm. Intuitively, with the $l_2$ norm, the actual value of the outlier plays a role. The larger it is, the more it pulls $x^*$ towards it.\n",
    "\n",
    "In contrast, with the $l_1$ norm, each outlier only \"counts\", independently of its actual value. If there are many inliers and only a few outliers, they will be unable to worsen the estimate substantially, no matter how far away they are from the true value.\n",
    "\n",
    "But if the number of outliers gets very large, the $l_1$ norm will walk away from the true solution, especially if the outliers are not \"symmetric\" around the true value. (Only the difference count between \"left\" and \"right\" outliers plays a role -- two more \"right\" outliers than \"left\" outliers make the median walk one to the right.) In order to further reduce the influence of outliers, we could try to discard them altogether.\n",
    "\n",
    "The idea of **maximum consensus** is to define an \"acceptance range\" $[-\\varepsilon, \\varepsilon]$ inside which an observation counts as inlier, otherwise it is an outlier. The goal is then to find the parameters $x^*$ which maximize the number of inliers. To formulate this, the indicator function $\\mathbb{1}(\\cdot)$ is typically used, where $\\mathbb{1}(c)$ is 1 if condition $c$ is true, and 0 otherwise. Then, the maximum consensus goal can be formulated as\n",
    "\n",
    "$$x^* = \\arg\\max_x \\sum_i \\mathbb{1}\\left( |x-b_i| \\leq \\varepsilon \\right),$$\n",
    "\n",
    "i.e., we are counting the inliers, $+1$ if inside, and $0$ if outside.\n",
    "\n",
    "However, to make it fit a bit more to our previous considerations, we can define it the opposite way: we count the outliers, i.e. $0$ if inside, and $+1$ if outside. Thus, we can achieve our goal by defining the *loss function*\n",
    "\n",
    "$$\\rho(r) = \\left\\{ \\begin{matrix}0&\\mbox{for}&|r|\\leq\\varepsilon\\\\1&\\mbox{else.}\\end{matrix}\\right.$$\n",
    "\n",
    "Here is a comparison of loss functions $\\rho(r)$ we used so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our new rho function.\n",
    "def rho_1d(r, eps):\n",
    "    return (np.abs(r)>eps).astype(float)\n",
    "\n",
    "x_comparison = np.linspace(-2,2,1000)\n",
    "plt.plot(x_comparison, x_comparison**2)\n",
    "plt.plot(x_comparison, np.abs(x_comparison))\n",
    "plt.plot(x_comparison, rho_1d(x_comparison, 1))\n",
    "plt.ylim(-.1,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we flattened the curve outside, we notice it is **not convex** anymore. This brings with it substantial trouble, as we will see. Because we hypothesized above that $4$ may be the correct value and $3$ and $5$ are still to be considered as inliers as well we will use $\\varepsilon=1.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_1d = 1.2\n",
    "for b in b_1d: plt.plot(x_vals_1d, rho_1d(x_vals_1d-b, eps_1d))\n",
    "plt.plot(x_vals_1d, (rho_1d(x_vals_1d-b_1d[:,np.newaxis], eps_1d)).sum(axis=0), \"black\")\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.ylim(-.5,6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has its minimum at the \"right\" location. It is a plateau, as in the $l_1$ case. However, as the single functions are not convex, their sum is neither. Apart from having steps, it has local minima. And of course, some of them may be identical to the global minimum, as shown in this example with two more points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1d_more = np.array([3,4,5,7,7.5,8])\n",
    "for b in b_1d_more: plt.plot(x_vals_1d, rho_1d(x_vals_1d-b, eps_1d))\n",
    "plt.plot(x_vals_1d, (rho_1d(x_vals_1d-b_1d_more[:,np.newaxis], eps_1d)).sum(axis=0), \"black\")\n",
    "plt.plot(b_1d_more, np.zeros_like(b_1d_more), \"bo\")\n",
    "plt.ylim(-.5,7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming a function to find the consensus.\n",
    "In the least squares case, we could use $x^*=$ `mean()`, in the $l_1$ case, $x^*=$ `median()`, but the maximum consensus case is less clear.\n",
    "\n",
    "The most obvious idea is to use a \"sweep type\" algorithm:\n",
    "- sweep the real line from left to right\n",
    "- at each $b_i-\\varepsilon$ (\"interval start\"), decrease the total score by 1\n",
    "- at each $b_i+\\varepsilon$ (\"interval end\"), increase the total score by 1\n",
    "- computing the cumulative sum, one can find the global minimum\n",
    "- for now, if there are several minima, just return one of them\n",
    "- to make it look nicer, return the \"center of the plateau\" (similar to the median case with an even number of values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_1d(b_vals, eps):\n",
    "    # Note tuple sort will always place the -1 event in front of the +1 event\n",
    "    # so the \"less or equal\" is implemented correctly if events coincide.\n",
    "    events = sorted([(b-eps,-1) for b in b_vals]+[(b+eps,1) for b in b_vals])\n",
    "    score = np.cumsum(np.array([e[1] for e in events]))\n",
    "    xs = np.argmin(score)  # Get one of the minima.\n",
    "    return (events[xs][0]+events[xs+1][0])/2  # Return the center of the plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for our simple 1D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in b_1d: plt.plot(x_vals_1d, rho_1d(x_vals_1d-b, eps_1d))\n",
    "plt.plot(x_vals_1d, (rho_1d(x_vals_1d-b_1d[:,np.newaxis], eps_1d)).sum(axis=0), \"black\")\n",
    "plt.plot(b_1d, np.zeros_like(b_1d), \"bo\")\n",
    "plt.axvline(x=cons_1d(b_1d, eps_1d), c=\"k\", ls=\"--\")\n",
    "plt.ylim(-.5,6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with the 1D random samples.\n",
    "Since we assumed our original signal to be normal distributed with $\\sigma=0.1$, we could set $\\varepsilon=2\\cdot\\sigma=0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_1d_random = 0.2\n",
    "def report_cons(inliers, outliers, eps):\n",
    "    both = np.concatenate((inliers, outliers))\n",
    "    plt.plot(inliers, rng.uniform(-.2,.2,len(inliers)), \"b.\")\n",
    "    plt.plot(outliers, rng.uniform(-.2,.2,len(outliers)), \"r.\")\n",
    "    plt.axvline(x=np.mean(both), c=\"silver\", ls=\"--\", label=\"mean\")\n",
    "    plt.axvline(x=np.median(both), c=\"dimgray\", ls=\"--\", label=\"median\")\n",
    "    plt.axvline(x=cons_1d(both, eps), c=\"black\", ls=\"--\", label=\"max cons\")\n",
    "    plt.legend()\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlim(-2,10)\n",
    "    print(\"inliers %d outliers %d mean %.3f median %.3f consensus %.3f\" %\\\n",
    "          (len(inliers), len(outliers), np.mean(both), np.median(both),\n",
    "           cons_1d(both, eps_1d_random)))\n",
    "report_cons(b_1d_meas, [], eps_1d_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, looks good with no noise. Now, for \"symmetric\" noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cons(b_1d_meas, b_1d_outl, eps_1d_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...looks good, too. For the asymmetric case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cons(b_1d_meas, b_1d_outl_a, eps_1d_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...looks still good. Now, for the case with the large amount of outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_cons(b_1d_meas, b_1d_more_outl_a, eps_1d_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the maximum consensus occurs close to $4$. That is pretty impressive, given that we have twice as many outliers than inliers.\n",
    "\n",
    "In order to figure out \"how sure\" we are about the maximum consensus, or the minimum in our score, we can plot the score over the entire range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals_mc = np.linspace(3,11,1000)\n",
    "all_b = np.concatenate((b_1d_meas, b_1d_more_outl_a))\n",
    "vals = np.sum([rho_1d(x_vals_mc - b, eps_1d_random) for b in all_b], axis=0)\n",
    "plt.plot(b_1d_meas, rng.uniform(-1,1,len(b_1d_meas))+55, \"b.\")  # +55 just for nice plot.\n",
    "plt.plot(b_1d_more_outl_a, rng.uniform(-1,1,len(b_1d_more_outl_a))+55, \"r.\")\n",
    "plt.plot(x_vals_mc, vals, c=\"k\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a model - the multidimensional case\n",
    "## Least squares estimation for the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, we will estimate a line. Our true line is $y = 0.5\\cdot x + 2$, but in order to escape notational clashes, we will use $(p,q)$ for the coordinates, so $q = 0.5 \\cdot p + 2$.\n",
    "\n",
    "In least squares, we generally want to find\n",
    "\n",
    "$$x^* = \\arg\\min_{x}|| Ax - b ||_2^2.$$\n",
    "\n",
    "where $x$ is the vector of unknowns. If we compute the gradient and set it to zero, similar to what we did in the 1D case above, we will get the well-known solution\n",
    "\n",
    "$$x^* = (A^\\top A)^{-1} A^\\top b$$\n",
    "\n",
    "(see e.g. chapter 3 in Hastie et al.). Note that $x$ are our unknowns to be estimated, not the $x$-values along the $x$-axis. That is, $x=(x_0, x_1)^\\top = (\\mbox{intercept},\\mbox{slope})^\\top$. Whereas for the points, let us say the point coordinates are $P_i = (p_i, q_i)$.\n",
    "\n",
    "For a given value $p_i$, along the horizontal axis, the line with intercept $x_0$ and slope $x_1$ has the value $x_1\\cdot p_i + x_0$, while the value of the point is $q_i$. So the residual vector is:\n",
    "\n",
    "$$ r = Ax-b = \\begin{bmatrix}1&p_0\\\\ 1&p_1\\\\ \\vdots & \\vdots\\\\ 1&p_{m-1} \\end{bmatrix}\\cdot\n",
    "   \\begin{bmatrix}x_0\\\\x_1\\end{bmatrix} - \\begin{bmatrix}q_0\\\\q_1\\\\ \\vdots \\\\q_{m-1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and we minimize $||r||_2^2$. Note that $r_i$ is not the perpendicular distance, but rather the ($q$-) axis parallel distance.  Instead of $(A^\\top A)^{-1} A^\\top$ one typically uses the [pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) which is typically available as `pinv` (note that although we do it here, one does not have to compute the inverse to solve the equation system).\n",
    "\n",
    "### First, introduce the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is our true model.\n",
    "def line(x):\n",
    "    return 0.5*x + 2\n",
    "\n",
    "# Generate samples: uniform along x, and for each x, additive Gaussian \"measurement\" noise.\n",
    "rng = np.random.default_rng(12345) # Restart.\n",
    "x_2d_vals = np.linspace(0, 10, num=100)\n",
    "n_2d_meas = 100\n",
    "inliers_x = rng.uniform(0,10,n_2d_meas)\n",
    "inliers_y = line(inliers_x) + rng.normal(0,0.1,n_2d_meas)\n",
    "inliers = np.vstack((inliers_x, inliers_y)).T\n",
    "\n",
    "plt.plot(x_2d_vals, line(x_2d_vals), c=\"lightgray\")\n",
    "plt.plot(inliers[:,0], inliers[:,1], \"b.\")\n",
    "plt.ylim(0,10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Symmetric outliers\": uniform samples.\n",
    "n_2d_outliers = 70\n",
    "outliers_x = rng.uniform(0,10,n_2d_outliers)\n",
    "outliers_y = line(outliers_x) + rng.uniform(-4,4,n_2d_outliers)\n",
    "outliers = np.vstack((outliers_x, outliers_y)).T\n",
    "plt.plot(x_2d_vals, line(x_2d_vals), c=\"lightgray\")\n",
    "plt.plot(inliers[:,0], inliers[:,1], \"b.\")\n",
    "plt.plot(outliers[:,0], outliers[:,1], \"r.\")\n",
    "plt.ylim(0,10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Asymmetric outliers\": uniform samples, all above.\n",
    "outliers_a_y = line(outliers_x) + rng.uniform(0,4,n_2d_outliers)\n",
    "outliers_a = np.vstack((outliers_x, outliers_a_y)).T\n",
    "plt.plot(x_2d_vals, line(x_2d_vals), c=\"lightgray\")\n",
    "plt.plot(inliers[:,0], inliers[:,1], \"b.\")\n",
    "plt.plot(outliers_a[:,0], outliers_a[:,1], \"r.\")\n",
    "plt.ylim(0,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the estimation results for the least squares case.\n",
    "\n",
    "First, let us see the solution for the system without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_l2(inl, outl):\n",
    "    both = np.concatenate((inl, outl))  # (m x 2) matrix of all point coordinates.\n",
    "\n",
    "    # Estimate.\n",
    "    A = np.vstack((np.ones(both.shape[0]), both[:,0])).T\n",
    "    intercept, slope = np.linalg.pinv(A) @ both[:,1]\n",
    "\n",
    "    # Report.\n",
    "    print(\"intercept %.3f slope %.3f\" % (intercept, slope))\n",
    "    plt.plot(x_2d_vals, line(x_2d_vals), c=\"lightgray\")\n",
    "    plt.plot(inl[:,0], inl[:,1], \"b.\")\n",
    "    plt.plot(outl[:,0], outl[:,1], \"r.\")\n",
    "    plt.plot(x_2d_vals, slope*x_2d_vals+intercept, c=\"k\")\n",
    "    plt.ylim(0,10)\n",
    "\n",
    "report_l2(inliers,np.empty((0,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with \"symmetric\" outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_l2(inliers, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with \"asymmetric\" outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_l2(inliers, outliers_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was as expected. The symmetric noise had a relatively small impact, but with all outliers above the line, the result was substantially shifted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least absolute deviations for the line\n",
    "\n",
    "Now it gets more interesting. Remember, we now have to do a $l_1$-norm minimization:\n",
    "\n",
    "$$x^* = \\arg\\min_x || Ax - b ||_1,$$\n",
    "\n",
    "which is exactly the same as\n",
    "\n",
    "$$x^* = \\arg\\min_x \\sum_{i=0}^{m-1} |a_i^\\top x-b_i|,$$\n",
    "\n",
    "were $a_i^\\top$ is the $i$-th row vector of $A$.\n",
    "\n",
    "How can we find this minimum? Being a sum of absolute values, the function is not differentiable. It is also not linear. However, since $|\\cdot|$ is a *convex function*, the *sum is convex*, too. So even though there is no closed-form solution, we know we can solve the problem efficiently.\n",
    "\n",
    "We can re-formulate the goal. Instead of $ \\min_x \\sum_{i=0}^{m-1} |a_i^\\top x-b_i|$ we write\n",
    "\n",
    "$$ \\min_{s,x} \\sum_{i=0}^{m-1} s_i,$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$ | a_i^\\top x - b_i | \\leq s_i .$$\n",
    "\n",
    "We can get rid of the absolute value by doubling the number of conditions,\n",
    "\n",
    "$$ \\begin{matrix} \\phantom{-}(a_i^\\top x - b_i) & \\leq & s_i & \\mbox{and}\\\\\n",
    "                 -(a_i^\\top x - b_i) & \\leq & s_i, & \\\\ \\end{matrix} $$\n",
    "                 \n",
    "and so, overall the problem is of the form\n",
    "\n",
    "$$ \\begin{array}{rl} \\mbox{minimize the value of:} & c^\\top x' \\\\\n",
    "               \\mbox{among all $x\\in\\mathbb{R}$ satisfying:} & A' x' \\leq b' \\end{array}$$\n",
    "                   \n",
    "where $x'$, $A'$ and $b'$ are formed from the above entities and \"$\\leq$\" between vectors is understood componentwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex optimization and linear programming\n",
    "The problem we have just formulated is called a **linear program**, and the methods to solve it are subsumed as **linear programming** (although it has nothing to do with \"programming\" as we understand it today).\n",
    "\n",
    "They are called *linear*, because both:\n",
    "- they have a linear **objective function** $f(x) = c^\\top x = c_0 x_0+ c_1 x_1 + \\ldots + c_{n-1} x_{n-1}$ *and*\n",
    "- the **constraints** $g(x) = A x \\leq b$ **are linear.**\n",
    "\n",
    "Problems like the one above are special cases of optimization (see Boyd & Vandenberghe):\n",
    "- **Mathematical optimization** is the general case:\n",
    "\n",
    "$$ \\begin{array}{rl} \\mbox{minimize the value of:} & f(x) \\\\\n",
    "               \\mbox{subject to:} & g(x) \\leq b, \\end{array}$$\n",
    "               \n",
    "- for a **linear program**, the objective and constraints must be linear, i.e. the components (say, $h_i$) of $f$ and $g$ must satisfy:\n",
    "\n",
    "$$h_i(\\alpha x + \\beta y) = \\alpha h_i(x) + \\beta h_i(y),$$\n",
    "\n",
    "- whereas for a **convex optimization problem**, objective and constraints must satisfy:\n",
    "\n",
    "$$h_i(\\alpha x + \\beta y) \\leq \\alpha h_i(x) + \\beta h_i(y).$$\n",
    "\n",
    "Linear programs and least-squares problems are special cases of convex optimization.\n",
    "\n",
    "Linear programming problems **can be solved efficiently**, the algorithms used are the famous **simplex method** due to George Dantzig (which however is not polynomial), the **ellipsoid method** (which is polynomial, but much slower in practice), and **interior point methods**, which are widely used nowadays. See also Matousek & GÃ¤rtner for a gentle introduction.\n",
    "\n",
    "To solve a linear programming problem, one uses typically a *solver*. This has the beauty of not having to program a solution to one's problem, instead it suffices to define the equations that describe one's problem.\n",
    "\n",
    "There are free solvers and commercial ones, most of which are free for academic users. We will use `cvxpy`, which is a modelling language for convex optimization problems. It includes some solvers, but also is able to call external ones, see the list [here](https://www.cvxpy.org/tutorial/advanced/index.html#choosing-a-solver). That is, if you use `cvxpy` and also install a academic version of the commercial solver `gurobi`, then calling `solve()` in `cvxpy` may call an internal solver for \"easy\" problems and `gurobi` for \"harder\" problems (as we will see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now lets develop the required formulas for our problem above. We have to minimize\n",
    "\n",
    "$$ \\min_{s,x} \\sum_{i=0}^{m-1} s_i,$$\n",
    "\n",
    "where in addition to the two original variables $x=(x_0,x_1)=(\\mbox{intercept},\\mbox{slope})$, we now also have the $s_i$ variables, one for each condition:\n",
    "\n",
    "$$ \\begin{matrix} \\phantom{-}(a_i^\\top x - b_i) & \\leq & s_i & \\mbox{and}\\\\\n",
    "                 -(a_i^\\top x - b_i) & \\leq & s_i, & \\\\ \\end{matrix} $$\n",
    "                 \n",
    "if we sort that into variables (l.h.s.) and constants (r.h.s.) we get\n",
    "\n",
    "$$ \\begin{matrix} \\phantom{-}a_i^\\top x - s_i & \\leq & \\phantom{-}b_i & \\mbox{and}\\\\\n",
    "                 -a_i^\\top x - s_i & \\leq & -b_i, & \\\\ \\end{matrix} $$\n",
    "\n",
    "and if we define, as above, $A$ and $b$ as\n",
    "\n",
    "$$ A = \\begin{bmatrix}1&p_0\\\\ 1&p_1\\\\ \\vdots & \\vdots\\\\ 1&p_{m-1} \\end{bmatrix},\n",
    "   b = \\begin{bmatrix}q_0\\\\q_1\\\\ \\vdots \\\\q_{m-1}\\end{bmatrix} $$\n",
    "   \n",
    "then our linear program is:\n",
    "\n",
    "$$ \\begin{array}{rl} \\mbox{minimize:} & \\sum_i s_i \\\\\n",
    "               \\mbox{subject to:} & \\phantom{-}A x - s \\leq b \\\\\n",
    "                                  & -A x - s \\leq -b\\end{array}$$\n",
    "\n",
    "and if you check the Python program below, this is pretty much how we can express it in `cvxpy`. Instead of the sum, one can alternatively write\n",
    "\n",
    "$$ \\begin{array}{rl} \\mbox{minimize:} & ||s||_1 \\\\\n",
    "               \\mbox{subject to:} & \\phantom{-}A x - s \\leq b \\\\\n",
    "                                  & -A x - s \\leq -b\\end{array}$$\n",
    "\n",
    "therefore, we *minimize the* $l_1$ *norm of* $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "\n",
    "def report_l1(inl, outl):\n",
    "    both = np.concatenate((inl, outl))  # (m x 2) matrix of all point coordinates.\n",
    "\n",
    "    # Estimate.\n",
    "    m = both.shape[0]\n",
    "    A = np.vstack((np.ones(m), both[:,0])).T\n",
    "    b = both[:,1]\n",
    "    # Setup problem for solver and solve.\n",
    "    x, s = cp.Variable(2), cp.Variable(m)\n",
    "    prob = cp.Problem(cp.Minimize(sum(s)),\n",
    "                      [ A @ x - s <=  b,\n",
    "                       -A @ x - s <= -b])\n",
    "    prob.solve(verbose=False)  # Set to true to see solver working.\n",
    "    \n",
    "    # Print and plot results.\n",
    "    intercept, slope = x.value\n",
    "    print(\"intercept %.3f slope %.3f\" % (intercept, slope))\n",
    "    plt.plot(x_2d_vals, line(x_2d_vals), c=\"lightgray\")\n",
    "    plt.plot(inl[:,0], inl[:,1], \"b.\")\n",
    "    plt.plot(outl[:,0], outl[:,1], \"r.\")\n",
    "    plt.plot(x_2d_vals, slope*x_2d_vals+intercept, c=\"k\")\n",
    "    plt.ylim(0,10)\n",
    "\n",
    "report_l1(inliers,np.empty((0,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_l1(inliers, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_l1(inliers, outliers_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we raise the number of outliers, this will break it, just as in the 1D case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_2d_more_outliers = 200\n",
    "more_outliers_x = rng.uniform(0,10,n_2d_more_outliers)\n",
    "more_outliers_a_y = line(more_outliers_x) + rng.uniform(0,4,n_2d_more_outliers)\n",
    "more_outliers_a = np.vstack((more_outliers_x, more_outliers_a_y)).T\n",
    "report_l1(inliers, more_outliers_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Consensus for the line\n",
    "\n",
    "Remember for maximum consensus, we have to count $+1$ if an observation is within bounds $\\pm\\varepsilon$. We then have to maximize the count of inliers\n",
    "\n",
    "$$x^* = \\arg\\max_x \\sum_i \\mathbb{1}\\left( |a_i^\\top x - b_i| \\leq \\varepsilon \\right).$$\n",
    "\n",
    "In the 1D case, we have expressed this already in an alternative way, namely as a minimization of the count of outliers. Define the $l_0$ (pseudo-) norm being *the number of non-zero elements*, i.e., $||s||_0$ is the count of elements of the vector $s$ which are non-zero. We now use $s$ as a *slack* variable, for which entry $s_i$ indicates the amount by which observation $i$ violates the $\\leq\\varepsilon$ constraint. Then, maximum consensus is the problem to\n",
    "\n",
    "$$ \\begin{array}{rl} \\mbox{minimize:} & ||s||_0 \\\\\n",
    "               \\mbox{subject to:} & | a_i^\\top x - b_i | \\leq \\varepsilon + s_i \\\\\n",
    "                                  & s_i \\geq 0.\\end{array}$$\n",
    "\n",
    "Thus, if observation $i$ is an inlier, it satisfies $| a_i^\\top x - b_i | \\leq \\varepsilon$, and the slack $s_i$ will be zero. However, observation $i$ is an outlier, $s_i > 0$ must hold, and this increases $||s||_0$ by $1$. Since the behaviour of $||s||_0$ is hard to handle, one typically uses the **big-M** method to re-formulate this as follows:\n",
    "\n",
    "$$ \\begin{array}{rl} \\mbox{minimize over $x\\in\\mathbb{R}^n$ and $z\\in\\{0,1\\}^m$:} & \\sum_i z_i \\\\\n",
    "               \\mbox{subject to:} & | a_i^\\top x - b_i | \\leq \\varepsilon + z_i\\cdot M, \\end{array}$$\n",
    "\n",
    "where $M$ is a sufficiently large *(big)* constant, and the variables $z_i$ are binary. They switch on or off the \"outlier status\" of the $i$-th observation. If $z_i=0$, the constraint $| a_i^\\top x - b_i | \\leq \\varepsilon$ must hold, and observation $i$ is an inlier. However, if $z_i=1$, $| a_i^\\top x - b_i | \\leq \\varepsilon + M$ must hold, and since $M$ is very large, this is always satisfied.\n",
    "\n",
    "Since the $z_i$ are required to be binary (or, they could also be from $\\mathbb{Z}$, we left the world of *linear programming* and entered the world of **Integer Programming (IP)**. More accurately, since we have to optimize a mixture of variable types, $x\\in\\mathbb{R}^n$ *and* $z\\in\\{0,1\\}^m$, this is a **Mixed Integer Program (MIP)**.\n",
    "\n",
    "Going from $\\mathbb{R}$ to $\\{0,1\\}$ or $\\mathbb{Z}$ seems like a little variation, but it changes everything. In general, there are no efficient ways to solve IP and MIP problems -- they are NP-hard. I.e., in general, one has to search the solution space. Well-known methods are *branch-and-bound*, *cutting planes*, and *branch-and-cut*. These methods are employed by solvers.\n",
    "\n",
    "Despite being hopeless in general, some IP/MIP are efficiently solvable, for others there are approximation algorithms (with guarantees) or efficient heuristics (without guarantees), see e.g. Conforti et al.. There are also links to combinatorial optimization, see Korte & Vygen. Approximation algorithms are discussed in Williamson & Shmoys.\n",
    "\n",
    "Side note on big-M: Although mathematically elegant, the \"big-M\" method is troubled, because if $M$ is made too small, the results will be wrong, and if $M$ is too large, convergence is slow and numerical instabilities may occur. This can be solved using *Mixed Logical Programming*, so in some solvers, one can use so-called *indicator constraints* of the form: $s_i \\Rightarrow cond$, \"if $s_i$ is true, condition $cond$ must hold\" as a replacement for the big-M formulation.\n",
    "\n",
    "In the following, we write down the equations from above in `cvxpy` notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_l0(inl, outl, eps, bigM):\n",
    "    both = np.concatenate((inl, outl))  # (m x 2) matrix of all point coordinates.\n",
    "\n",
    "    # Estimate\n",
    "    m = both.shape[0]\n",
    "    A = np.vstack((np.ones(m), both[:,0])).T\n",
    "    b = both[:,1]\n",
    "    x, z = cp.Variable(2), cp.Variable(m, boolean=True)  # Note: boolean=True.\n",
    "    prob = cp.Problem(cp.Minimize(sum(z)),\n",
    "                      [ A @ x - z * bigM <= eps + b,\n",
    "                       -A @ x - z * bigM <= eps - b])\n",
    "    prob.solve(verbose=True)  # Set to true to watch the solver working.\n",
    "    \n",
    "    # Print and plot results.\n",
    "    intercept, slope = x.value\n",
    "    print(\"intercept %.3f slope %.3f\" % (intercept, slope))\n",
    "    plt.plot(x_2d_vals, line(x_2d_vals), c=\"lightgray\")\n",
    "    plt.plot(inl[:,0], inl[:,1], \"b.\")\n",
    "    plt.plot(outl[:,0], outl[:,1], \"r.\")\n",
    "    plt.plot(x_2d_vals, slope*x_2d_vals+intercept, c=\"k\")\n",
    "    plt.ylim(0,10)\n",
    "\n",
    "# Uncomment the next line to run (may take two minutes).\n",
    "#report_l0(inliers, more_outliers_a, eps=0.2, bigM=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Leaving the robust optimization topic!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex optimization example: *model selection*, aka *shrinkage*\n",
    "\n",
    "Now we used the problem of robust estimation to motivate least squares, least absolute deviation, and maximum consensus. To find a solution, we used an analytical solution (least squares), linear programming (least absolute deviation), and mixed integer programming (maximum consensus).\n",
    "\n",
    "Although we motivated all this using the case of robust estimation, these techniques are important in their own right and are applied in numerous areas. This section is an example where you would encounter quadratic programming, the second-order sibling of linear programming.\n",
    "\n",
    "The *model selection problem* occurs almost everywhere: if we want to explain data using a model, we should make the model sufficiently complex so that it is able to describe the \"main characteristics\" of the data. However, we should not make it overly complex, because then it will learn by heart the particular samples we have.\n",
    "\n",
    "Here, we will illustrate this with fitting a polynomial to data. Intuitively, the \"model complexity\" is tied to the degree of the polynomial, or the number of monomials used to describe the polynomial (see below).\n",
    "\n",
    "Methods for *model selection* are also termed *shrinkage methods*. See e.g. chap. 3.4 in Hastie et al.\n",
    "\n",
    "We will look at four different methods to select our model:\n",
    "- variation of the degree of the polynomial,\n",
    "- ridge regression,\n",
    "- the lasso,\n",
    "- monomial selection via IP.\n",
    "\n",
    "*Note: in the following, the intercept is not estimated. If we would estimate this as well, then we would have an additional \"$1$\" column in A, and $x_0$ would be the corresponding coefficient to be estimated. However then, in the ridge regression and lasso approaches below, one would have to replace $||x||_2$ and $||x||_1$ by $||x_{1:k-1}||_2$ and $||x_{1:k-1}||_1$, respectively, because $x_0$ depends on the choice of the coordinate system and it would make no sense to suppress large values. Equivalently, one can estimate the mean of $x$ and $b$ and work with centered inputs from there on, see Hastie et al., chap. 4.3.1 and 3.4.2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is our true model.\n",
    "def model(x):\n",
    "    return x**3 - 3*x\n",
    "\n",
    "# Generate samples: uniform along x, and for each x, additive Gaussian \"measurement\" noise.\n",
    "rng = np.random.default_rng(1234) # Restart.\n",
    "x_2d_vals = np.linspace(-2,2,num=100)\n",
    "n_2d_train, n_2d_test = 10, 10\n",
    "data_x = rng.uniform(-2,2,n_2d_train+n_2d_test)\n",
    "data_y = model(data_x) + rng.normal(0,0.2,data_x.size)\n",
    "data = np.vstack((data_x, data_y)).T\n",
    "train = data[:n_2d_train,:]\n",
    "test  = data[n_2d_train:,:]\n",
    "\n",
    "plt.plot(x_2d_vals, model(x_2d_vals), c=\"lightgray\")\n",
    "plt.plot(train[:,0], train[:,1], \"b.\", label=\"train\")\n",
    "plt.plot(test[:,0], test[:,1], \"r.\", label=\"test\")\n",
    "plt.legend()\n",
    "ylim=(-4,4)\n",
    "plt.ylim(ylim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a suitable model via degree variation\n",
    "\n",
    "As above, we use least-squares to find the parameters of the polynomial\n",
    "\n",
    "$$x^* = \\arg\\min_{x}|| Ax - b ||_2^2.$$\n",
    "\n",
    "which leads to the well-known solution\n",
    "\n",
    "$$x^* = (A^\\top A)^{-1} A^\\top b.$$\n",
    "\n",
    "Varying the \"model complexity\" is done by changing the degree of the polynomial which means that our $x$ vector has more or less entries. For example, if the degree of the polynomial is $1$, $x$ is a scalar, and $A$ has only one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powers of polynomials used for the fit: x**1, x**2,...\n",
    "powers = range(1,10)\n",
    "\n",
    "# The A matrix, each column is a x^i of the x coordinates.\n",
    "A = np.vstack(tuple([train[:,0]**p for p in powers])).T\n",
    "A_test = np.vstack(tuple([test[:,0]**p for p in powers])).T\n",
    "\n",
    "# This is the least squares estimate.\n",
    "coeff = np.linalg.pinv(A) @ train[:,1]\n",
    "print(\"coefficients =\", coeff)\n",
    "    \n",
    "# Plot result. Helper matrix M is like A, but for \"all\" x values.\n",
    "M = np.vstack(tuple([x_2d_vals**p for p in powers])).T\n",
    "plt.plot(x_2d_vals, model(x_2d_vals), c=\"lightgray\")  # The model.\n",
    "plt.plot(x_2d_vals, M@coeff, c=\"k\")  # The estimate.\n",
    "plt.plot(train[:,0], train[:,1], \"b.\")  # The training data used for the estimate.\n",
    "plt.plot(test[:,0], test[:,1], \"r.\")  # The test data, just for visualization.\n",
    "plt.ylim(ylim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LS where degree can be set via slider.\n",
    "import ipywidgets as ipyw\n",
    "def plot_scan(max_columns):\n",
    "    coeff = np.linalg.pinv(A[:,:max_columns]) @ train[:,1]\n",
    "    plt.plot(x_2d_vals, model(x_2d_vals), c=\"lightgray\")  # The model.\n",
    "    plt.plot(x_2d_vals, M[:,:max_columns]@coeff, c=\"k\")  # The estimate.\n",
    "    plt.plot(train[:,0], train[:,1], \"b.\")  # The training data used for the estimate.\n",
    "    plt.plot(test[:,0], test[:,1], \"r.\")  # The test data, just for visualization.\n",
    "    plt.title(\" | \".join(\"%.2f\" % c for c in coeff))\n",
    "    plt.ylim(ylim)\n",
    "ipyw.interact(plot_scan, max_columns = ipyw.IntSlider(min=1, max=A.shape[1], value=A.shape[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/test vs. degree.\n",
    "errors = []\n",
    "for max_columns in range(len(powers),0,-1):\n",
    "    coeff = np.linalg.pinv(A[:,:max_columns]) @ train[:,1]  # LS for this degree.\n",
    "    train_error = np.mean((A[:,:max_columns]@coeff - train[:,1])**2)  # MSE\n",
    "    test_error = np.mean((A_test[:,:max_columns]@coeff - test[:,1])**2)\n",
    "    errors.append((train_error, test_error))\n",
    "degree_errors = np.array(errors)\n",
    "degree_min_i = np.argmin(degree_errors[:,1])\n",
    "print(\"minimum test error at degree = %d\" % powers[A.shape[1]-1-degree_min_i])\n",
    "print(\"coefficients = \", np.linalg.pinv(A[:,:A.shape[1]-degree_min_i]) @ train[:,1])\n",
    "print(\"test error (MSE) = %.4f\" % degree_errors[degree_min_i,1])\n",
    "\n",
    "plt.axvline(x=degree_min_i, c=\"gray\", ls=\"--\")\n",
    "plt.plot(degree_errors[:,0], c=\"blue\", label=\"train\")\n",
    "plt.plot(degree_errors[:,1], c=\"red\", label=\"test\")\n",
    "plt.xticks(range(len(powers)), range(len(powers),0,-1));\n",
    "plt.legend()\n",
    "train_test_ylim = (0,8)\n",
    "plt.ylim(train_test_ylim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a suitable model via ridge regression\n",
    "\n",
    "Instead finding the solution for all different degrees (and dimensions of $A$), we \"suppress\" model complexity by forcing $x$ to be small (in terms of the squared $l_2$ norm). This is called *regularization*.\n",
    "\n",
    "$$x^* = \\arg\\min_{x}|| Ax - b ||_2^2 + \\lambda ||x||_2^2.$$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter. Fortunately, this modification integrates nicely with standard least-squares, and the closed-form solution is\n",
    "\n",
    "$$x^* = (A^\\top A + \\lambda I)^{-1} A^\\top b,$$\n",
    "\n",
    "so all there is to do is to add $\\lambda$ to the main diagonal. This is a standard trick to remove singularities. Also, in surveying this is typically used as a soft constraint to \"fix parameters by pseudo observation\". And, in ML estimation, it can be nicely motivated by a prior distribution for x.\n",
    "\n",
    "Varying the \"model complexity\" now means to play with $\\lambda$. Intuitively, if it is 0, we obtain the standard least-squares solution, with the most complex model. If it grows, the model will become simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression.\n",
    "def ls_ridge(A, b, la):\n",
    "    return np.linalg.solve(A.T @ A + np.eye(A.shape[1])*la, A.T @ b)\n",
    "\n",
    "coeff = ls_ridge(A, train[:,1], 0.0001) # Set lambda here.\n",
    "print(coeff)\n",
    "\n",
    "# Plot result.\n",
    "plt.plot(x_2d_vals, model(x_2d_vals), c=\"lightgray\")\n",
    "plt.plot(x_2d_vals, M@coeff, c=\"k\")\n",
    "plt.plot(train[:,0], train[:,1], \"b.\")\n",
    "plt.plot(test[:,0], test[:,1], \"r.\")\n",
    "plt.ylim(ylim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: estimation vs. lambda.\n",
    "lambda_logrange = (-11, 2)\n",
    "def plot_scan(Lambda):\n",
    "    coeff = ls_ridge(A, train[:,1], Lambda)\n",
    "    plt.plot(x_2d_vals, model(x_2d_vals), c=\"lightgray\")\n",
    "    plt.plot(x_2d_vals, M@coeff, c=\"k\")\n",
    "    plt.plot(train[:,0], train[:,1], \"b.\")\n",
    "    plt.plot(test[:,0], test[:,1], \"r.\")\n",
    "    plt.title(\" | \".join((\"%.2f\" % c) if abs(c) > 1e-2 else \"****\" for c in coeff))\n",
    "    plt.ylim(ylim)\n",
    "ipyw.interact(plot_scan, Lambda = ipyw.FloatLogSlider(\n",
    "    min=lambda_logrange[0], max=lambda_logrange[1], value=lambda_logrange[0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error vs. lambda.\n",
    "lambdas = np.logspace(*lambda_logrange,100)\n",
    "errors, components = [], []\n",
    "for la in lambdas:\n",
    "    coeff = ls_ridge(A, train[:,1], la)  # Ridge regression for this lambda.\n",
    "    train_error = np.mean((A@coeff - train[:,1])**2)  # MSE\n",
    "    test_error = np.mean((A_test@coeff - test[:,1])**2)\n",
    "    errors.append((train_error, test_error))\n",
    "    components.append((abs(coeff)>1e-2).sum())  # Also store the number of non-zero components.\n",
    "ridge_errors = np.array(errors)\n",
    "ridge_min_i = np.argmin(ridge_errors[:,1])\n",
    "print(\"minimum test error at lambda = %.3f\" % lambdas[ridge_min_i])\n",
    "print(\"coefficients = \", ls_ridge(A, train[:,1], lambdas[ridge_min_i]))\n",
    "print(\"test error (MSE) = %.4f\" % ridge_errors[ridge_min_i,1])\n",
    "\n",
    "plt.axvline(x=lambdas[ridge_min_i], c=\"gray\", ls=\"--\")\n",
    "plt.plot(lambdas, components, \":\", c=\"lightgray\", label=\"components\")\n",
    "plt.plot(lambdas, ridge_errors[:,0], c=\"blue\", label=\"train\")\n",
    "plt.plot(lambdas, ridge_errors[:,1], c=\"red\", label=\"test\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "train_test_ylim = (0,10)\n",
    "plt.ylim(train_test_ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a suitable model via the lasso method\n",
    "\n",
    "Now, we use the $l_2$ norm for the residuals, but the $l_1$ norm to \"suppress\" model complexity:\n",
    "\n",
    "$$x^* = \\arg\\min_{x}|| Ax - b ||_2^2 + \\lambda ||x||_1.$$\n",
    "\n",
    "where as before, $\\lambda$ is a hyperparameter. This does not integrate nicely in the least squares approach. What we obtain is a quadratic programming problem, which we will hand over to the solver. $\\lambda$ plays the same role as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso. Not linear programming, but rather quadratic programming (and convex optimization).\n",
    "def loss_fn(A, b, x):\n",
    "    return cp.norm2(A @ x - b)**2\n",
    "def regularizer(x):\n",
    "    return cp.norm1(x)\n",
    "def objective_fn(A, b, x, Lambda):\n",
    "    return loss_fn(A, b, x) + Lambda * regularizer(x)\n",
    "def mse(A, b, x):\n",
    "    return (1.0 / A.shape[0]) * loss_fn(A, b, x).value\n",
    "\n",
    "def ls_lasso(A, b, Lambda):\n",
    "    x = cp.Variable(A.shape[1])\n",
    "    #lambd = cp.Parameter(nonneg=True)\n",
    "    problem = cp.Problem(cp.Minimize(objective_fn(A, b, x, Lambda)))\n",
    "    problem.solve(solver=\"GUROBI\", verbose=False)\n",
    "    return x.value\n",
    "ls_lasso(A, train[:,1], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: estimation vs. lambda.\n",
    "def plot_scan(Lambda):\n",
    "    coeff = ls_lasso(A, train[:,1], Lambda)\n",
    "    plt.plot(x_2d_vals, model(x_2d_vals), c=\"lightgray\")\n",
    "    plt.plot(x_2d_vals, M@coeff, c=\"k\")\n",
    "    c2 = coeff.copy()\n",
    "    c2[abs(c2)<1e-2]=0\n",
    "    plt.plot(x_2d_vals, M@c2, \"--\", c=\"k\")\n",
    "    plt.plot(train[:,0], train[:,1], \"b.\")\n",
    "    plt.plot(test[:,0], test[:,1], \"r.\")\n",
    "    plt.title(\" | \".join((\"%.2f\" % c) if abs(c) > 1e-2 else \"****\" for c in coeff))\n",
    "    plt.ylim(ylim)\n",
    "ipyw.interact(plot_scan, Lambda = ipyw.FloatLogSlider(\n",
    "    min=lambda_logrange[0], max=lambda_logrange[1], value=lambda_logrange[0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error vs. lambda.\n",
    "errors, components = [], []\n",
    "for la in lambdas:\n",
    "    coeff = ls_lasso(A, train[:,1], la)  # Ridge regression for this lambda.\n",
    "    train_error = np.mean((A@coeff - train[:,1])**2)  # MSE\n",
    "    test_error = np.mean((A_test@coeff - test[:,1])**2)\n",
    "    errors.append((train_error, test_error))\n",
    "    components.append((abs(coeff)>1e-2).sum())  # Also store the number of non-zero components.\n",
    "    \n",
    "lasso_errors = np.array(errors)\n",
    "lasso_min_i = np.argmin(lasso_errors[:,1])\n",
    "print(\"minimum test error at lambda = %.3f\" % lambdas[lasso_min_i])\n",
    "print(\"coefficients = \", ls_lasso(A, train[:,1], lambdas[lasso_min_i]))\n",
    "print(\"test error (MSE) = %.4f\" % lasso_errors[lasso_min_i,1])\n",
    "\n",
    "plt.axvline(x=lambdas[lasso_min_i], c=\"gray\", ls=\"--\")\n",
    "plt.plot(lambdas, components, \":\", c=\"lightgray\", label=\"components\")\n",
    "plt.plot(lambdas, lasso_errors[:,0], c=\"blue\", label=\"train\")\n",
    "plt.plot(lambdas, lasso_errors[:,1], c=\"red\", label=\"test\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.ylim(train_test_ylim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of test errors of ridge regression vs. lasso.\n",
    "plt.plot(lambdas, ridge_errors[:,0], c=\"lightblue\", label=\"train (ridge)\")\n",
    "plt.plot(lambdas, ridge_errors[:,1], c=\"lightsalmon\", label=\"test (ridge)\")\n",
    "plt.plot(lambdas, lasso_errors[:,0], c=\"blue\", label=\"train (lasso)\")\n",
    "plt.plot(lambdas, lasso_errors[:,1], c=\"red\", label=\"test (lasso)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.ylim(train_test_ylim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the coefficients: ridge regression vs. lasso.\n",
    "For ridge regression and lasso, we have a continuous parameter $\\lambda$ to influence the model complexity. One can draw the components of $x$ vs. $\\lambda$, sometimes called *regularization path*.\n",
    "\n",
    "Comparing the results of ridge regression vs. lasso, one can see the tendency of lasso to obtain zero coefficients, or *sparse* results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge: coefficients vs. lambda.\n",
    "ridge_results = np.array([ ls_ridge(A, train[:,1], la) for la in lambdas ])\n",
    "for c in range(ridge_results.shape[1]):\n",
    "    plt.plot(lambdas, ridge_results[:,c], label=powers[c])\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge: coefficients vs. lambda.\n",
    "plt.axhline(y=-2, c=\"gray\", ls=\"--\")\n",
    "ridge_results = np.array([ ls_ridge(A, train[:,1], la) for la in lambdas ])\n",
    "for c in range(ridge_results.shape[1]):\n",
    "    plt.plot(lambdas, np.log10(np.abs(ridge_results[:,c])+1e-7), label=powers[c])\n",
    "plt.legend()\n",
    "ylim_log_coeff=(-7.5,3)\n",
    "plt.ylim(ylim_log_coeff)\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso: coefficients vs. lambda.\n",
    "lasso_results = np.array([ ls_lasso(A, train[:,1], la) for la in lambdas ])\n",
    "for c in range(lasso_results.shape[1]):\n",
    "    plt.plot(lambdas, lasso_results[:,c], label=powers[c])\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso: coefficients vs. lambda.\n",
    "plt.axhline(y=-2, c=\"gray\", ls=\"--\")\n",
    "lasso_results = np.array([ ls_lasso(A, train[:,1], la) for la in lambdas ])\n",
    "for c in range(lasso_results.shape[1]):\n",
    "    plt.plot(lambdas, np.log10(np.abs(lasso_results[:,c])+1e-7), label=powers[c])\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.ylim(ylim_log_coeff);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both coefficient sets in one figure.\n",
    "for c in range(ridge_results.shape[1]):\n",
    "    plt.plot(lambdas, np.log10(np.abs(ridge_results[:,c])+1e-7), c=\"silver\")\n",
    "for c in range(lasso_results.shape[1]):\n",
    "    plt.plot(lambdas, np.log10(np.abs(lasso_results[:,c])+1e-7), c=\"k\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylim(ylim_log_coeff);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monomial selection via IP\n",
    "\n",
    "Let's come back to the first technique we looked at, *degree variation*. We selected the number of components $1\\leq c\\leq k$ and then computed the least squares estimate for a polynomial of degree $c$.\n",
    "\n",
    "For example, in our case, the reference model is of degree $3$, and we obtained the best result using a polynomial of degree $3$. However, the reference model only contains the monomials $x$ and $x^3$, whereas our fit contains *all monomials up to degree* $3$, i.e., $x$, $x^2$, $x^3$.\n",
    "\n",
    "So instead of requiring a maximum degree for the polynomial, we could require a maximum number of components (monomials). For example, if our reference model would contain only the odd monomials $x$, $x^3$, $x^5$, $x^7$, we could obtain a good fit using a polynomial with 4 components, but we would not achieve a good fit with a polynomial of degree 4, since this would contain $x$, $x^2$, $x^3$, $x^4$. Thus, we would include \"useless\" monomials $x^2$, $x^4$, while being unable to model the effects of $x^5$ and $x^7$.\n",
    "\n",
    "The best fit with at most $c$ components can be elegantly formulated using integers (a mixed integer quadratic program):\n",
    "\n",
    "$$ \\begin{array}{rl} \\mbox{minimize:} & ||Ax - b||_2^2 \\\\\n",
    "               \\mbox{subject to:} & | x_i | \\leq z_i\\cdot M \\\\\n",
    "                                  & \\sum_i z_i \\leq c,\\end{array}$$\n",
    "\n",
    "$x_i\\in\\mathbb{R}$, $z_i\\in \\{0,1\\}$, again using the \"big-M\" formulation. If $z_i$ is $0$, the component $x_i$ is forced to $0$ in the least squares estimation, otherwise, it is unconstrained. This can be directly formulated in `cvxpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute solutions with 1,...,c components.\n",
    "# If you encounter problems with this bigM constant, try to play with it. For me, this value\n",
    "# worked on Linux, but not on Windows, a kind of worrying result. But after all, bigM\n",
    "# is known for being numerically problematic. The solution would be to re-formulate it in terms\n",
    "# of \"indicator constraints\".\n",
    "bigM = 5000\n",
    "k_comp_solutions = [ None ]\n",
    "for c in range(1, A.shape[1]+1):\n",
    "    x, z = cp.Variable(A.shape[1]), cp.Variable(A.shape[1], boolean=True)\n",
    "    prob = cp.Problem(cp.Minimize(cp.norm2(A @ x - train[:,1])**2),\n",
    "                      [-bigM*z <= x,\n",
    "                       x <= bigM*z,\n",
    "                       cp.sum(z) <= c ])\n",
    "    prob.solve(verbose=False)\n",
    "    k_comp_solutions.append(x.value)\n",
    "\n",
    "# k component solutions via slider.\n",
    "import ipywidgets as ipyw\n",
    "def plot_scan(max_components):\n",
    "    coeff = k_comp_solutions[max_components]\n",
    "    plt.plot(x_2d_vals, model(x_2d_vals), c=\"lightgray\")  # The model.\n",
    "    plt.plot(x_2d_vals, M@coeff, c=\"k\")  # The estimate.\n",
    "    plt.plot(train[:,0], train[:,1], \"b.\")  # The training data used for the estimate.\n",
    "    plt.plot(test[:,0], test[:,1], \"r.\")  # The test data, just for visualization.\n",
    "    plt.title(\" | \".join((\"%.2f\" % c) if abs(c) > 1e-2 else \"****\" for c in coeff))\n",
    "    plt.ylim(ylim)\n",
    "ipyw.interact(plot_scan, max_components = ipyw.IntSlider(min=1, max=A.shape[1], value=A.shape[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/test vs. number of components.\n",
    "errors = []\n",
    "for max_columns in range(1, A.shape[1]+1):\n",
    "    coeff = k_comp_solutions[max_columns]\n",
    "    train_error = np.mean((A@coeff - train[:,1])**2)  # MSE\n",
    "    test_error = np.mean((A_test@coeff - test[:,1])**2)\n",
    "    errors.append((train_error, test_error))\n",
    "component_errors = np.array(errors)\n",
    "best_components = np.argmin(component_errors[:,1])+1\n",
    "print(\"minimum test error at #components = %d\" % best_components)\n",
    "print(\"coefficients = \", k_comp_solutions[best_components])\n",
    "print(\"test error (MSE) = %.4f\" % component_errors[best_components-1,1])\n",
    "\n",
    "plt.axvline(x=A.shape[1]-best_components, c=\"gray\", ls=\"--\")\n",
    "plt.plot(range(A.shape[1]-1,-1,-1), component_errors[:,0], c=\"blue\", label=\"train\")\n",
    "plt.plot(range(A.shape[1]-1,-1,-1), component_errors[:,1], c=\"red\", label=\"test\")\n",
    "plt.xticks(range(A.shape[1]), range(A.shape[1],0,-1))\n",
    "plt.legend()\n",
    "plt.ylim(train_test_ylim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A \"typical\" integer programming problem\n",
    "\n",
    "Often, this is combinatorial optimization which can be expressed as an IP problem. Problems like:\n",
    "- assignment problem (connect taxi drivers to customers)\n",
    "- transportation/ transshipment problem\n",
    "- knapsack problem\n",
    "- bin packing problem\n",
    "- cutting stock problem\n",
    "- ...\n",
    "\n",
    "But also in Geoinformatics:\n",
    "- generalization\n",
    "- label placement\n",
    "- maximum a posteriori estimation in case of discrete random variables (see Chap. 13.5, *MAP as a linear optimization problem* in Koller & Friedman).\n",
    "\n",
    "Here we will pick the following one:\n",
    "\n",
    "## The uncapacitated facility location problem\n",
    "\n",
    "We have clients $D$ (demand) and tentative outlet locations $F$ (facilities). If client $j$ goes to outlet $i$, this is associated with a cost $c_{ij}$, here we will use the Euclidean distance. So the client will always go to the closest outlet. If we want happy clients, we would open up as many outlets as possible (i.e. at each tentative location). However, to open an outlet up, we have to build it, so there is a one-time fixed cost $f_i$ to open outlet $i$. Now the goal is to minimize the total cost.\n",
    "\n",
    "Formulated as an IP problem, we have the variables:\n",
    "- $y_i$ which are binary variables telling if outlet $i$ is opened up ($1$) or not ($0$).\n",
    "- $x_{ij}$ which are binary variables telling if customer $j$ goes to outlet $i$.\n",
    "\n",
    "Then, the optimization problem is:\n",
    "\n",
    "$$ \\begin{array}{rlll} \\mbox{minimize:} & \\sum_i f_i y_i + \\sum_{ij} c_{ij} x_{ij}\\\\\n",
    "               \\mbox{subject to:} & \\sum_i x_{ij} = 1 & \\forall j &\n",
    "                                      \\mbox{(each client goes to exactly one outlet)}\\\\\n",
    "                                  & x_{ij} \\leq y_i & \\forall i,j &\n",
    "                                      \\mbox{(a client can only go to an outlet that is open)}\\\\\n",
    "                                  & x_{ij}, y_i \\in \\{0,1\\}.\n",
    "                                  \\end{array}$$\n",
    "\n",
    "This is a typical IP formulation: the binary variables are used as switches.\n",
    "\n",
    "The *uncapacitated facility location problem* is a very well researched problem (see Korte & Vygen, chap. 22.1, and Williamson & Shmoys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "from matplotlib.collections import LineCollection\n",
    "rng = np.random.default_rng(12345) # Restart.\n",
    "n_factories = 40\n",
    "n_demands = 80\n",
    "factory_locations, demand_locations = rng.uniform(0,1,(n_factories, 2)), rng.uniform(0,1,(n_demands, 2))\n",
    "\n",
    "# Random factory costs.\n",
    "factory_max_cost = 20\n",
    "factory_costs_1 = rng.uniform(0,factory_max_cost,n_factories) \n",
    "factory_costs_2 = factory_max_cost - factory_costs_1 \n",
    "\n",
    "def evaluate(f_cost = 0.5, radius = 0.1):\n",
    "    # Setup possible pairs, according to max_radius.\n",
    "    tree = cKDTree(factory_locations)\n",
    "    close_factories = tree.query_ball_point(demand_locations, radius)\n",
    "    edges = np.array([ (i,n) for i, cf in enumerate(close_factories) for n in cf ], dtype=int)\n",
    "    incoming_indices, j = [], 0\n",
    "    for nlist in close_factories:\n",
    "        l = len(nlist)\n",
    "        if l > 0:\n",
    "            incoming_indices.append((j,j+l))\n",
    "            j += l\n",
    "\n",
    "    # Compute edge costs, use Euclidean distance.\n",
    "    edge_costs = np.array([ np.linalg.norm(demand_locations[d]-factory_locations[f]) for d,f in edges ])\n",
    "\n",
    "    # Factory costs are a linear combination of the two costs.\n",
    "    factory_costs = (1.0-f_cost)*factory_costs_1 + f_cost*factory_costs_2\n",
    "\n",
    "    # Solve problem.\n",
    "    yi = cp.Variable(n_factories, boolean=True)\n",
    "    xij = cp.Variable(len(edges), boolean=True)\n",
    "    prob = cp.Problem(cp.Minimize(factory_costs.T @ yi + edge_costs @ xij),\n",
    "                      [ cp.sum(xij[f:t]) == 1 for f,t in incoming_indices ] +\n",
    "                      [ xij <= yi[edges[:,1]] ])\n",
    "\n",
    "    prob.solve(verbose=False)\n",
    "\n",
    "    # Plot all possible edges.\n",
    "    line_coll_data = np.array([ [tuple(demand_locations[d]), tuple(factory_locations[f])] for d, f in edges ])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.add_collection(LineCollection(line_coll_data, colors=\"gainsboro\", zorder=0))\n",
    "    ax.add_collection(LineCollection(line_coll_data[xij.value > 0], colors=\"blue\", zorder=1))\n",
    "    # Plot factory and demand locations.\n",
    "    plt.scatter(demand_locations[:,0], demand_locations[:,1], c=\"white\", edgecolors=\"k\", s=50)\n",
    "    plt.scatter(factory_locations[:,0], factory_locations[:,1], c=factory_costs,\n",
    "                cmap=\"magma\", vmin=0, vmax=factory_max_cost, s=100, marker=\"v\")\n",
    "    plt.scatter(factory_locations[:,0][yi.value>0], factory_locations[:,1][yi.value>0], c=factory_costs[yi.value>0],\n",
    "                cmap=\"magma\", vmin=0, vmax=factory_max_cost, s=300, marker=\"s\", edgecolors=\"k\")\n",
    "\n",
    "ipyw.interact(evaluate, f_cost = ipyw.FloatSlider(min=0, max=1, step=0.05, value=0.5, continuous_update=False),\n",
    "              radius = ipyw.FloatSlider(min=0.05, max=1.0, step=0.01, value=0.05, continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "- Boyd & Vandenberghe: Convex Optimization, [free web download from the authors](https://web.stanford.edu/~boyd/cvxbook/).\n",
    "- Conforti, Cornuejols, Zambelli: Integer Programming, [Springer](https://www.springer.com/gp/book/9783319110073).\n",
    "- Hastie, Tibshirani, Friedman: The Elements of Statistical Learning, [free web download from the authors](https://web.stanford.edu/~hastie/ElemStatLearn/).\n",
    "- Koller & Friedman: Probabilistic Graphical Models, [MIT Press](https://mitpress.ublish.com/book/probabilistic-graphical-models).\n",
    "- Korte & Vygen: Combinatorial Optimization (also available as German book), [available online for LUH members](https://www.springer.com/gp/book/9783662560389).\n",
    "- Matousek & GÃ¤rtner: Understanding and Using Linear Programming, [available online for LUH members](https://www.springer.com/de/book/9783540306979#otherversion=9783540307174).\n",
    "- Williamson & Shmoys: The Design of Approximation Algorithms, [free web download from the authors](http://www.designofapproxalgs.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries to run this notebook\n",
    "\n",
    "- If you don't have Python you will typically install the [anaconda](https://www.anaconda.com/products/individual) distribution. This includes python3, numpy, scipy, matplotlib, etc. On Linux, `pip install` works too.\n",
    "- To run the notebook itself, start *jupyter notebook* or *JupyterLab* (also included in the anaconda distribution).\n",
    "- If the interactive sliders do not work, you will have to install ipywidgets, see [here](https://ipywidgets.readthedocs.io/en/stable/user_install.html). It's just a `conda install` or a `pip install`.\n",
    "- If cvxpy is missing, see [here](https://www.cvxpy.org/install/index.html). This is also just a `conda install` (or a `pip install`).\n",
    "- The gurobi solver used for some of the cells is a commercial product, but there is a free academic license. You may install it using `conda install gurobi`. Once you have installed it, cvxpy will automatically select it for some of the problems, or if you request it by `problem.solve(solver=\"GUROBI\")`. After installation, you will have to get an *individual academic license* to make it work, follow the instructions [here](https://www.gurobi.com/academia/academic-program-and-licenses/)."
   ]
  }
 ],
 "metadata": {
  "copyright": "(c) Claus Brenner, 17 DEC 2020",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
